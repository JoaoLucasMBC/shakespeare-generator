{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Using LLM assitance**\n",
    "\n",
    "If we want a lightweigth model that can generate Shakespearian sonnets, we realized that the model by itself is challenging. While it manages to get the goals we had set up for the train and test set, its ability to create generalized structure is limited.\n",
    "\n",
    "However, this open an opportunity to incorporate the thing that everyone is talking about (but without just asking it to solve a task for us and trusting the black box): LLMs!\n",
    "\n",
    "## How can we do it?\n",
    "\n",
    "Instead of asking the LLM to generate a sonnet, we can provide it with the generated sonnet from our model and ask it to correct it. Eliminate duplicate words, strange structures, and assist line breaking. In this way, we can be more in control of the LLM, asking it to not alter our original sonnet, but do what it does best: work with patterns. Because, in the end of the day, a model like Gemini probably already \"read\" all sonnets written by humanity..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "import torch\n",
    "\n",
    "from model import TransformerModel\n",
    "from utils import generate_sonnet_sampling, clean_generated_text\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model/checkpoint_epoch_200_valloss_1.9365.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21556/2830344480.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location='cpu'))\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(\n",
    "    vocab_size=4000, \n",
    "    seq_len=256, \n",
    "    embedding_dim=256, \n",
    "    num_heads=8, \n",
    "    num_layers=6, \n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "model_path = 'model/checkpoint_epoch_200_valloss_1.9365.pth'\n",
    "\n",
    "# Load\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    print(f\"Model loaded from {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution that yielded some of the better results while reading was using sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='tokenizer/my_tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thy their known ruthless lament their their these muse reveal me vain imposed history's of sightcauldronion from thee. figure first within the these unwritten my of Decoright from the absent, did speak toos. \n",
      ". akatch use is from the upon myju fall their violet I baseamand reason light of monarchs bold to fall away lies wanting ardor gulf naught \n",
      " Nor Marsing and bright Pardoning bow de sight lent. \n",
      " Thus on Death bow to nature call, these lines, gracing her head richness \n",
      " For grow from those pe Ardor of grasp of hollow glory, adorned confess my feeble they have victor fate my far from their guest, claim I think beneath the sun'll declare; qu \n",
      " Here song amidst grace flares gates. \n",
      " Why fearch thrives, seek treasure voy and remorselessiness die of stars lost, Shall fade, yearning totholo doessers, reviled.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "start_text = \"Thy\"\n",
    "raw = generate_sonnet_sampling(model, sp, start_text, max_length=256, device='cuda')\n",
    "generated_sonnet = clean_generated_text(raw, pad_token='<PAD>')\n",
    "print(generated_sonnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Gemini:\n",
      "\n",
      "Thy known ruthless lament, these muse reveal, vain, imposed history's sight, a cauldron from thee.\n",
      "Figure first within unwritten Decoright, from the absent, did speak.\n",
      "A catch, use is from upon my fall, their violet, I base my reason; light of monarchs bold, to fall away, lies wanting ardor, a gulf naught.\n",
      "Nor Mars' bright pardoning bow, sight lent.\n",
      "Thus, on Death's bow to nature's call, these lines, gracing her head's richness,\n",
      "grow from those; ardor's grasp of hollow glory, adorned, confess my feeble; they have victor fate, my far from their guest, claim I think beneath the sun declare.\n",
      "Here song, amidst grace, flares gates.\n",
      "Why search thrives, seek treasure; voyaging remorselessness, die of stars lost, shall fade, yearning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the use of the API\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Make our prompt here\n",
    "prompt = f\"\"\"\n",
    "You are an expert and a teacher in Shakespearing sonnets. Your goal is to take your student's sonnet and help him polish it. You need to pay attention to:\n",
    "* Duplicate words\n",
    "* Repetitive phrases\n",
    "* Line breaks\n",
    "* Slight mispells\n",
    "* Words that were clearly stuck together\n",
    "\n",
    "You are NOT allowed to change the student's words or their meaning, that will make him sad!\n",
    "\n",
    "Your output should be the new sonnet. No explanations, no anything more or less.\n",
    "\n",
    "Here is the student's sonnet:\n",
    "{generated_sonnet}\n",
    "\"\"\"\n",
    "\n",
    "generation_config = genai.GenerationConfig(\n",
    "    max_output_tokens=256,\n",
    "    temperature=1.0,\n",
    ")\n",
    "\n",
    "# Use our prompt \n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "\n",
    "response = model.generate_content(prompt,\n",
    "                                  generation_config=generation_config)\n",
    "\n",
    "print(\"Response from Gemini:\")\n",
    "print()\n",
    "print(response.candidates[0].content.parts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thy their known ruthless lament their their these muse reveal me vain imposed history's of sightcauldronion from thee. figure first within the these unwritten my of Decoright from the absent, did speak toos. \n",
      ". akatch use is from the upon myju fall their violet I baseamand reason light of monarchs bold to fall away lies wanting ardor gulf naught \n",
      " Nor Marsing and bright Pardoning bow de sight lent. \n",
      " Thus on Death bow to nature call, these lines, gracing her head richness \n",
      " For grow from those pe Ardor of grasp of hollow glory, adorned confess my feeble they have victor fate my far from their guest, claim I think beneath the sun'll declare; qu \n",
      " Here song amidst grace flares gates. \n",
      " Why fearch thrives, seek treasure voy and remorselessiness die of stars lost, Shall fade, yearning totholo doessers, reviled.\n"
     ]
    }
   ],
   "source": [
    "print(generated_sonnet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-shakespeare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
