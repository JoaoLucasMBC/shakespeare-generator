{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Using LLM assitance**\n",
    "\n",
    "If we want a lightweigth model that can generate Shakespearian sonnets, we realized that the model by itself is challenging. While it manages to get the goals we had set up for the train and test set, its ability to create generalized structure is limited.\n",
    "\n",
    "However, this open an opportunity to incorporate the thing that everyone is talking about (but without just asking it to solve a task for us and trusting the black box): LLMs!\n",
    "\n",
    "## How can we do it?\n",
    "\n",
    "Instead of asking the LLM to generate a sonnet, we can provide it with the generated sonnet from our model and ask it to correct it. Eliminate duplicate words, strange structures, and assist line breaking. In this way, we can be more in control of the LLM, asking it to not alter our original sonnet, but do what it does best: work with patterns. Because, in the end of the day, a model like Gemini probably already \"read\" all sonnets written by humanity..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaolucasmbc/mambaforge/envs/nlp-shakespeare/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "import torch\n",
    "\n",
    "from model import TransformerModel\n",
    "from utils import generate_sonnet_sampling, clean_generated_text\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model/checkpoint_epoch_200_valloss_1.9365.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90387/2830344480.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location='cpu'))\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(\n",
    "    vocab_size=4000, \n",
    "    seq_len=256, \n",
    "    embedding_dim=256, \n",
    "    num_heads=8, \n",
    "    num_layers=6, \n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "model_path = 'model/checkpoint_epoch_200_valloss_1.9365.pth'\n",
    "\n",
    "# Load\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    print(f\"Model loaded from {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution that yielded some of the better results while reading was using sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='tokenizer/my_tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thy both most ensnared our honor our honor honor friendship respect lies read lies in our being- our love forever lives lies our thee to the honor witht th love stoneive learn to discern core within through life by labory itst phrases in its sacred She ourt\n",
      " Yet death to plague he belong me reside, So simple's deceptive dost black lies, Over granted scholars by its amongmp find believ gaze find its decreeyis skillownys worth outshine free. \n",
      " Ens fail peace no more our destiny entwined in ourselves inv haless repent, ourselves concealed within our age no stage lead; \n",
      " Seducing our artist in this ground extreme; \n",
      " Speak restmeri solace glare, our secret, with its profan bloom that flourishes both grief maintain thus, better tainteds vast archive shall kindly Nature'll needs're fashion has madey expression to move by tears and moonlit air.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "start_text = \"Thy\"\n",
    "raw = generate_sonnet_sampling(model, sp, start_text, max_length=256, device='cuda')\n",
    "generated_sonnet = clean_generated_text(raw, pad_token='<PAD>')\n",
    "print(generated_sonnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Gemini:\n",
      "\n",
      "Thy ensnared honor, friendship, respect, lies read\n",
      "In our being; our love forever lives, lies\n",
      "In thee; the honor, love, stone I've learned to discern,\n",
      "Core within, through life's labor, its phrases, sacred.\n",
      "\n",
      "Yet death may plague; he belongs, resides in me.\n",
      "So simple's deceptive, black lies,\n",
      "Over-granted scholars find, believe, gaze, find\n",
      "Its decree; its skill, own worth outshines free.\n",
      "\n",
      "Ensnare peace; no more our destiny entwined,\n",
      "In ourselves, unless repent; ourselves concealed\n",
      "Within our age, no stage leads;\n",
      "Seducing our art in this extreme ground.\n",
      "\n",
      "Speak, rest; mere solace glares; our secret,\n",
      "With its profane bloom, flourishes; both grief\n",
      "Maintained, vast archive shall kindly nature's fashion\n",
      "Express; to move by tears and moonlit air.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the use of the API\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Make our prompt here\n",
    "prompt = f\"\"\"\n",
    "You are an expert and a teacher in Shakespearing sonnets. Your goal is to take your student's sonnet and help him polish it. You need to pay attention to:\n",
    "* Duplicate words\n",
    "* Repetitive phrases\n",
    "* Line breaks (always has to be a sonnet)\n",
    "* Slight mispells\n",
    "* Lack of rhymes\n",
    "* Words that were clearly stuck together\n",
    "\n",
    "You are NOT allowed to change the student's words or their meaning, that will make him sad!\n",
    "\n",
    "Your output should be the new sonnet. No explanations, no anything more or less.\n",
    "\n",
    "Here is the student's sonnet:\n",
    "{generated_sonnet}\n",
    "\"\"\"\n",
    "\n",
    "generation_config = genai.GenerationConfig(\n",
    "    max_output_tokens=256,\n",
    "    temperature=1.0,\n",
    ")\n",
    "\n",
    "# Use our prompt \n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "\n",
    "response = model.generate_content(prompt,\n",
    "                                  generation_config=generation_config)\n",
    "\n",
    "print(\"Response from Gemini:\")\n",
    "print()\n",
    "print(response.candidates[0].content.parts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thy their known ruthless lament their their these muse reveal me vain imposed history's of sightcauldronion from thee. figure first within the these unwritten my of Decoright from the absent, did speak toos. \n",
      ". akatch use is from the upon myju fall their violet I baseamand reason light of monarchs bold to fall away lies wanting ardor gulf naught \n",
      " Nor Marsing and bright Pardoning bow de sight lent. \n",
      " Thus on Death bow to nature call, these lines, gracing her head richness \n",
      " For grow from those pe Ardor of grasp of hollow glory, adorned confess my feeble they have victor fate my far from their guest, claim I think beneath the sun'll declare; qu \n",
      " Here song amidst grace flares gates. \n",
      " Why fearch thrives, seek treasure voy and remorselessiness die of stars lost, Shall fade, yearning totholo doessers, reviled.\n"
     ]
    }
   ],
   "source": [
    "print(generated_sonnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating 3 sonnets\n",
    "\n",
    "Prompts always have 8 words to start. \n",
    "\n",
    "All sonnets came from [this data source](https://www.readwritethink.org/sites/default/files/resources/lesson_images/lesson830/sonnet-links.html). The first 3 were used in this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the use of the API\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Make our prompt here\n",
    "prompt = f\"\"\"\n",
    "You are an expert and a teacher in Shakespearing sonnets. Your goal is to take your student's sonnet and help him polish it. You need to pay attention to:\n",
    "* Duplicate words\n",
    "* Repetitive phrases\n",
    "* Line breaks (always has to be a sonnet)\n",
    "* Slight mispells\n",
    "* Lack of rhymes\n",
    "* Words that were clearly stuck together\n",
    "\n",
    "You are NOT allowed to change the student's words or their meaning, that will make him sad!\n",
    "\n",
    "Your output should be the new sonnet. No explanations, no anything more or less.\n",
    "\n",
    "Here is the student's sonnet: \n",
    "\"\"\"\n",
    "\n",
    "generation_config = genai.GenerationConfig(\n",
    "    max_output_tokens=256,\n",
    "    temperature=1.0,\n",
    ")\n",
    "\n",
    "# Use our prompt \n",
    "llm = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earth has not anything to show more fair: flattery find respect for  to multitudellinting disdain \n",
      " Bright and beauty, yet green lost Unfair to common to grow to the tales  too, remain, clear by errors isolationlain. \n",
      " veins Sings zenithing, fairly the chains guise, defy shatterly speak provoke for my rem,aning eclipses increase by reprieve, unfa Time run thy devotion increase, has captures to a shadow graced by Time's fades away free. blely winter cannot straying rotlit grace one thou shalt Death wouldfere falls to grayears would with ease. \n",
      " Feed. looms death to Reviv eternal pride, extremeived toose it pleasess accord? ve lost where life may cease. \n",
      " Who granted have sworn whispers both to find dark ve'tis cause creature.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "start_text = \"Earth has not anything to show more fair:\"\n",
    "raw = generate_sonnet_sampling(model, sp, start_text, max_length=256, device='cuda')\n",
    "generated_sonnet = clean_generated_text(raw, pad_token='<PAD>')\n",
    "print(generated_sonnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Gemini:\n",
      "\n",
      "Earth has not anything to show more fair:\n",
      "Flattery finds respect for multitudinous disdain.\n",
      "Bright beauty, yet green lost, unfair, common to grow.\n",
      "The tales remain, clear by errors, isolation lain.\n",
      "\n",
      "Veins sing zenithing, fairly the chains guise.\n",
      "Defy shatteringly; speak, provoke my remaining eclipses.\n",
      "Increase by reprieve, unfaded time runs.\n",
      "Thy devotion increases; captures a shadow graced by time's fades away free.\n",
      "\n",
      "Bleak winter cannot stray; rotting grace, one thou shalt.\n",
      "Death would prefer falls to gray years with ease.\n",
      "Feed. Death looms to revive eternal pride, extreme.\n",
      "It pleases; accords? We've lost where life may cease.\n",
      "\n",
      "Who granted have sworn whispers both to find.\n",
      "Dark veils; it is cause; creature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = llm.generate_content(prompt + generated_sonnet,\n",
    "                                  generation_config=generation_config)\n",
    "\n",
    "print(\"Response from Gemini:\")\n",
    "print()\n",
    "print(response.candidates[0].content.parts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death, be not proud, though some have called ; arnished by these unwritten from with scroll- Witness, xict the not, from their devoid concern piece for is solace, \"In its Se him wellspringed prayer far understand outwardnoc of time has consume these lines on both pen utter eye grow and ardor Slowly come to truth' hands inspire their words on fields of blameistend bidnor march of burden? \n",
      " This in thus: of imprint of self is so bright when beautiful sight, and found in chest; \n",
      " Ardor the tale of time had oneadeoroake. \n",
      " Whoso restrains? st weed, though. \n",
      " Through is not rise eyes; don, lent or earthing wisdom from me defiant? ephemeral? wo disarray can hide we, all has flower I cannot restore nores compel vice is through duty? intoed honor anthology.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "start_text = \"Death, be not proud, though some have called\"\n",
    "raw = generate_sonnet_sampling(model, sp, start_text, max_length=256, device='cuda')\n",
    "generated_sonnet = clean_generated_text(raw, pad_token='<PAD>')\n",
    "print(generated_sonnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Gemini:\n",
      "\n",
      "Death, be not proud, though some have called thee\n",
      "adorned by these unwritten scrolls —\n",
      "Witness not, from their devoid concern,\n",
      "a solace, \"In its wellspring,\" prayer, far understand\n",
      "outward reach of time. Consume these lines;\n",
      "Both pen, and eye, grow old, and ardor\n",
      "slowly comes to truth. Hands inspire\n",
      "their words on fields of blame.  Attend; nor march of burden?\n",
      "\n",
      "This imprint of self is so bright,\n",
      "when beautiful, a sight, and found in the chest;\n",
      "Ardor, the tale of time, had oneadeoroake.\n",
      "Whoso restrains?  A weed, though.\n",
      "\n",
      "Through eyes, not risen, lent or earthing wisdom;\n",
      "Defiant? Ephemeral? Woe! Disarray\n",
      "cannot hide. We, all, have flowered.\n",
      "I cannot restore, nor compel. Vice is through duty? Intoed honor, anthology.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = llm.generate_content(prompt + generated_sonnet,\n",
    "                                  generation_config=generation_config)\n",
    "\n",
    "print(\"Response from Gemini:\")\n",
    "print()\n",
    "print(response.candidates[0].content.parts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death, be not proud, How do I love thee? Let me count I be path without me hence parts love should make so smooth own then, \n",
      " Letified To keep wisdom'er was self, me serve from myself, thoughere wiltedmplayed How, I could fall now tainted, with care. \n",
      " Con veil I fulfill your nimble me with tenschool judge back solveestified torment thank lack to self, swiftly trace. worms beyond compare? By lips and love from ills deception sings in chastedors, held in thy freedom both dark veil in air, be true lingers weep me can join. \n",
      " To keep and broken on, though defam? Would both, abandon my beloved bow? \n",
      " It last. \n",
      " Abandoned and come by actions tainted, when you both strength beyond held, love forever lay: merit. \n",
      " Passion, dost and its faults, thus, in?\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "start_text = \"Death, be not proud, How do I love thee? Let me count \"\n",
    "raw = generate_sonnet_sampling(model, sp, start_text, max_length=256, device='cuda')\n",
    "generated_sonnet = clean_generated_text(raw, pad_token='<PAD>')\n",
    "print(generated_sonnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Gemini:\n",
      "\n",
      "Death, be not proud. How do I love thee? Let me count.\n",
      "I be path without me. Hence, parts love should make so smooth.\n",
      "Own then, letified to keep wisdom. 'Twas self, me serve from myself.\n",
      "Though ere wilted, I played. How I could fall, now tainted, with care.\n",
      "Con veil I fulfill. Your nimble me, with ten, school judge.\n",
      "Back, solve, letified torment. Thank lack to self, swiftly trace.\n",
      "Worms beyond compare? By lips and love from ills, deception sings.\n",
      "In chaste doors, held in thy freedom. Both dark veil in air, be true.\n",
      "Lingers. Weep. Me can join. To keep and broken on, though defamed?\n",
      "Would both abandon my beloved bow? It last.\n",
      "Abandoned, and come by actions tainted. When you both strength beyond held,\n",
      "Love forever lay: merit. Passion, dost and its faults, thus, in?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = llm.generate_content(prompt + generated_sonnet,\n",
    "                                  generation_config=generation_config)\n",
    "\n",
    "print(\"Response from Gemini:\")\n",
    "print()\n",
    "print(response.candidates[0].content.parts[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-shakespeare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
