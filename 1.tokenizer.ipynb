{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(460, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sonnet Number</th>\n",
       "      <th>Variation Number</th>\n",
       "      <th>Variation Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Desire for growth in loveliest of beings, &lt;LIN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>In the fairest of beings, we crave increase, &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>For fairest beings, we yearn for more to grace...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>As forty winters carve their icy lines on your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>When winter's hand has etched its stories on y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sonnet Number  Variation Number  \\\n",
       "0              1                 1   \n",
       "1              1                 2   \n",
       "2              1                 3   \n",
       "3              2                 1   \n",
       "4              2                 2   \n",
       "\n",
       "                                      Variation Text  \n",
       "0  Desire for growth in loveliest of beings, <LIN...  \n",
       "1  In the fairest of beings, we crave increase, <...  \n",
       "2  For fairest beings, we yearn for more to grace...  \n",
       "3  As forty winters carve their icy lines on your...  \n",
       "4  When winter's hand has etched its stories on y...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/diffused_sonnets_2.csv')\n",
    "\n",
    "df['Variation Text'] = df['Variation Text'].str.replace('\\n', ' <LINE> ')\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Simple Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./data/sonnets_train.txt\n",
      "  input_format: \n",
      "  model_prefix: ./tokenizer/my_tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 4000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <LINE>\n",
      "  user_defined_symbols: <PAD>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ./data/sonnets_train.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 460 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <LINE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <PAD>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=296757\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9579% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=54\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999579\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 460 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=162388\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 15508 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 460\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 9499\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 9499 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5955 obj=13.2879 num_tokens=18997 num_tokens/piece=3.19009\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5080 obj=11.3511 num_tokens=19212 num_tokens/piece=3.78189\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4369 obj=11.3912 num_tokens=19919 num_tokens/piece=4.55917\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4358 obj=11.349 num_tokens=19923 num_tokens/piece=4.57159\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: ./tokenizer/my_tokenizer.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: ./tokenizer/my_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare the training data for SentencePiece: join all sonnets with newlines\n",
    "combined_text = '\\n'.join(df['Variation Text'].tolist())\n",
    "\n",
    "# 2. Write combined text to a temporary file (SentencePiece expects a file input for training)\n",
    "with open('./data/sonnets_train.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(combined_text)\n",
    "\n",
    "# 3. Train SentencePiece tokenizer from the file\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='./data/sonnets_train.txt',\n",
    "    model_prefix='./tokenizer/my_tokenizer',\n",
    "    vocab_size=4000, \n",
    "    user_defined_symbols=['<LINE>', '<PAD>']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1989, 30, 3528, 12, 2144, 104, 15, 1489, 6, 5, 3, 45, 239, 49, 9, 7, 236, 50, 1091, 20, 201, 6, 5, 3, 101, 21, 3718, 1284, 381, 6, 244, 76, 76, 2732, 6, 5, 3, 25, 639, 15, 2003, 16, 3482, 8, 5, 3, 61, 18, 6, 699, 16, 13, 60, 130, 145, 6, 5, 3, 64, 3015, 13, 92, 9, 7, 257, 35, 143, 79, 1434, 1892, 6, 5, 3, 2540, 20, 863, 12, 21, 610, 15, 953, 9, 7, 1998, 6, 5, 3, 105, 1992, 2635, 13, 60, 143, 6, 262, 182, 246, 8, 5, 3, 146, 6, 80, 10, 227, 9, 7, 216, 517, 328, 2139, 290, 6, 5, 3, 19, 31, 3033, 20, 83, 15, 451, 12, 62, 43, 504, 6, 5, 3, 24, 13, 60, 1084, 6, 214, 13, 152, 6, 5, 3, 19, 1705, 54, 977, 6, 158, 707, 974, 7, 3432, 8, 5, 3, 474, 67, 1095, 51, 10, 119, 6, 71, 46, 1894, 706, 757, 608, 6, 5, 3, 229, 230, 612, 87, 9, 7, 5, 2963, 6, 32, 417, 17, 99, 8, 5, 3, 5, 3]\n"
     ]
    }
   ],
   "source": [
    "# 4. Load the trained tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('./tokenizer/my_tokenizer.model')\n",
    "\n",
    "# 5. Tokenize each sonnet in the DataFrame\n",
    "def tokenize_sonnet(text):\n",
    "    return sp.encode_as_ids(text)\n",
    "\n",
    "df['tokenized_sonnet'] = df['Variation Text'].apply(tokenize_sonnet)\n",
    "\n",
    "# Example: print tokenized pieces for the first sonnet\n",
    "print(df.loc[0, 'tokenized_sonnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of newlines in Variation Text: 14.826086956521738\n"
     ]
    }
   ],
   "source": [
    "df.loc[0, \"tokenized_sonnet\"].count(sp.encode('<LINE>')[0])\n",
    "df.loc[0, \"Variation Text\"].count('\\n')\n",
    "\n",
    "# Get the average amount of  slash n of all Variation Text\n",
    "avg_newlines = df['Variation Text'].str.count('<LINE>').mean()\n",
    "\n",
    "print(f\"Average number of newlines in Variation Text: {avg_newlines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([460, 256])\n"
     ]
    }
   ],
   "source": [
    "def pad_tokenized_sonnets(tokenized_sonnets, pad_id=-1, max_len=256):\n",
    "\n",
    "    # Truncate each token list to max_len, then convert to tensor\n",
    "    token_tensors = [torch.tensor(tokens[:max_len]) for tokens in tokenized_sonnets]\n",
    "    \n",
    "    # Pad sequences to the max length in the batch (‚â§ max_len)\n",
    "    padded_tensor = pad_sequence(token_tensors, batch_first=True, padding_value=pad_id)\n",
    "    \n",
    "    # If padding length is less than max_len, pad extra manually\n",
    "    if padded_tensor.size(1) < max_len:\n",
    "        pad_size = max_len - padded_tensor.size(1)\n",
    "        pad_tensor = torch.zeros((padded_tensor.size(0), pad_size), dtype=padded_tensor.dtype)\n",
    "        padded_tensor = torch.cat([padded_tensor, pad_tensor], dim=1)\n",
    "    \n",
    "    return padded_tensor\n",
    "\n",
    "# Pad the tokenized sonnets\n",
    "padded_sonnets = pad_tokenized_sonnets(df['tokenized_sonnet'].tolist(), pad_id=sp.piece_to_id('<PAD>'))\n",
    "\n",
    "# Example: print the shape of the padded tensor\n",
    "print(padded_sonnets.shape)  # Should be (batch_size, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input X and target y:\n",
    "# X: tokenized sonnets padded to (batch_size, 256)\n",
    "X = padded_sonnets\n",
    "\n",
    "# y: X shifted left by 1 (next token prediction), pad last token with 0 or ignore_index\n",
    "y = torch.zeros_like(X)\n",
    "y[:, :-1] = X[:, 1:]\n",
    "y[:, -1] = sp.pad_id()  # or any padding token id to ignore last token prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1989,   30, 3528,  ...,    0,    0,    0],\n",
       "         [  24,   10,  210,  ...,    0,    0,    0],\n",
       "         [  42,  210,  104,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [  24,  335,  753,  ...,    0,    0,    0],\n",
       "         [  24,  186,    5,  ...,    0,    0,    0],\n",
       "         [  24, 2270,  240,  ...,    0,    0,    0]]),\n",
       " tensor([[  30, 3528,   12,  ...,    0,    0,   -1],\n",
       "         [  10,  210,  104,  ...,    0,    0,   -1],\n",
       "         [ 210,  104, 1489,  ...,    0,    0,   -1],\n",
       "         ...,\n",
       "         [ 335,  753,    6,  ...,    0,    0,   -1],\n",
       "         [ 186,    5,  491,  ...,    0,    0,   -1],\n",
       "         [2270,  240,    6,  ...,    0,    0,   -1]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save X and y tensors to a file\n",
    "torch.save((X, y), './data/sonnets_data.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Rhyme Tags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rhyme scheme tokens\n",
    "rhyme_classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
    "\n",
    "def add_rhyme_tokens(sonnet_text, rhyme_scheme=\"ABABCDCDEFEFGG\", target_lines=14):\n",
    "    \"\"\"\n",
    "    Insert rhyme tokens at the end of each line of the sonnet based on rhyme scheme.\n",
    "    Pads or truncates sonnets to `target_lines` lines.\n",
    "    \n",
    "    sonnet_text: str, full sonnet text with lines separated by <LINE> tokens\n",
    "    rhyme_scheme: str, e.g. Shakespeare‚Äôs 14-line sonnet scheme\n",
    "    target_lines: int, number of lines to pad/truncate to (default 14)\n",
    "    \n",
    "    Returns: sonnet text with rhyme tokens inserted at line ends, padded/truncated to target_lines\n",
    "    \"\"\"\n",
    "    # Split sonnet into lines\n",
    "    lines = sonnet_text.split('<LINE>')\n",
    "    lines = [line.strip() for line in lines if line.strip()]  # Remove empty lines\n",
    "    \n",
    "    # Pad with empty lines if too short\n",
    "    if len(lines) < target_lines:\n",
    "        lines += [''] * (target_lines - len(lines))\n",
    "    # Truncate if too long\n",
    "    elif len(lines) > target_lines:\n",
    "        lines = lines[:target_lines]\n",
    "    \n",
    "    # Adjust rhyme scheme length to target_lines\n",
    "    rhyme_scheme = rhyme_scheme[:target_lines]\n",
    "    \n",
    "    # Append rhyme token at end of each line\n",
    "    lines_with_rhyme = [\n",
    "        line + f\" <rhyme_{rhyme_scheme[i]}>\" for i, line in enumerate(lines)\n",
    "    ]\n",
    "    \n",
    "    # Re-join with <LINE>\n",
    "    return \" <LINE> \".join(lines_with_rhyme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sonnet Number</th>\n",
       "      <th>Variation Number</th>\n",
       "      <th>Variation Text</th>\n",
       "      <th>tokenized_sonnet</th>\n",
       "      <th>rhyme_sonnet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Desire for growth in loveliest of beings, &lt;LIN...</td>\n",
       "      <td>[1989, 30, 3528, 12, 2144, 104, 15, 1489, 6, 5...</td>\n",
       "      <td>Desire for growth in loveliest of beings, &lt;rhy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>In the fairest of beings, we crave increase, &lt;...</td>\n",
       "      <td>[24, 10, 210, 104, 15, 1489, 6, 112, 1018, 750...</td>\n",
       "      <td>In the fairest of beings, we crave increase, &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>For fairest beings, we yearn for more to grace...</td>\n",
       "      <td>[42, 210, 104, 1489, 6, 112, 763, 30, 109, 16,...</td>\n",
       "      <td>For fairest beings, we yearn for more to grace...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>As forty winters carve their icy lines on your...</td>\n",
       "      <td>[101, 30, 820, 2751, 1242, 47, 5, 3647, 228, 5...</td>\n",
       "      <td>As forty winters carve their icy lines on your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>When winter's hand has etched its stories on y...</td>\n",
       "      <td>[91, 276, 9, 7, 163, 144, 5, 1271, 43, 1378, 5...</td>\n",
       "      <td>When winter's hand has etched its stories on y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sonnet Number  Variation Number  \\\n",
       "0              1                 1   \n",
       "1              1                 2   \n",
       "2              1                 3   \n",
       "3              2                 1   \n",
       "4              2                 2   \n",
       "\n",
       "                                      Variation Text  \\\n",
       "0  Desire for growth in loveliest of beings, <LIN...   \n",
       "1  In the fairest of beings, we crave increase, <...   \n",
       "2  For fairest beings, we yearn for more to grace...   \n",
       "3  As forty winters carve their icy lines on your...   \n",
       "4  When winter's hand has etched its stories on y...   \n",
       "\n",
       "                                    tokenized_sonnet  \\\n",
       "0  [1989, 30, 3528, 12, 2144, 104, 15, 1489, 6, 5...   \n",
       "1  [24, 10, 210, 104, 15, 1489, 6, 112, 1018, 750...   \n",
       "2  [42, 210, 104, 1489, 6, 112, 763, 30, 109, 16,...   \n",
       "3  [101, 30, 820, 2751, 1242, 47, 5, 3647, 228, 5...   \n",
       "4  [91, 276, 9, 7, 163, 144, 5, 1271, 43, 1378, 5...   \n",
       "\n",
       "                                        rhyme_sonnet  \n",
       "0  Desire for growth in loveliest of beings, <rhy...  \n",
       "1  In the fairest of beings, we crave increase, <...  \n",
       "2  For fairest beings, we yearn for more to grace...  \n",
       "3  As forty winters carve their icy lines on your...  \n",
       "4  When winter's hand has etched its stories on y...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"rhyme_sonnet\"] = df[\"Variation Text\"].apply(add_rhyme_tokens)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/sonnets_rhymes_train.txt', 'w', encoding='utf-8') as f:\n",
    "    for sonnet in df['rhyme_sonnet']:\n",
    "        f.write(sonnet + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./data/sonnets_rhymes_train.txt\n",
      "  input_format: \n",
      "  model_prefix: ./tokenizer/my_rhyme_tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 4000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <LINE>\n",
      "  user_defined_symbols: <PAD>\n",
      "  user_defined_symbols: <rhyme_A>\n",
      "  user_defined_symbols: <rhyme_B>\n",
      "  user_defined_symbols: <rhyme_C>\n",
      "  user_defined_symbols: <rhyme_D>\n",
      "  user_defined_symbols: <rhyme_E>\n",
      "  user_defined_symbols: <rhyme_F>\n",
      "  user_defined_symbols: <rhyme_G>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ./data/sonnets_rhymes_train.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 460 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <LINE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <PAD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <rhyme_A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <rhyme_B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <rhyme_C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <rhyme_D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <rhyme_E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <rhyme_F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <rhyme_G>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=307185\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9593% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=54\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999593\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 460 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=168796\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 15487 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 460\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 9486\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 9486 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5975 obj=14.496 num_tokens=18995 num_tokens/piece=3.17908\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5099 obj=12.6668 num_tokens=19229 num_tokens/piece=3.77113\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4366 obj=12.71 num_tokens=19958 num_tokens/piece=4.57123\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4357 obj=12.6516 num_tokens=19967 num_tokens/piece=4.58274\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: ./tokenizer/my_rhyme_tokenizer.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: ./tokenizer/my_rhyme_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "rhyme_tokens = [f\"<rhyme_{c}>\" for c in rhyme_classes]\n",
    "\n",
    "user_defined_symbols = ['<LINE>', \"<PAD>\"] + rhyme_tokens\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='./data/sonnets_rhymes_train.txt',\n",
    "    model_prefix='./tokenizer/my_rhyme_tokenizer',\n",
    "    vocab_size=4000,\n",
    "    user_defined_symbols=user_defined_symbols,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load the trained tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('./tokenizer/my_rhyme_tokenizer.model')\n",
    "\n",
    "# 5. Tokenize each sonnet in the DataFrame\n",
    "def tokenize_sonnet(text):\n",
    "    return sp.encode_as_ids(text)\n",
    "\n",
    "df['tokenized_rhyme_sonnet'] = df['rhyme_sonnet'].apply(tokenize_sonnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([460, 277])\n"
     ]
    }
   ],
   "source": [
    "max_len = df['tokenized_rhyme_sonnet'].apply(len).max()\n",
    "\n",
    "# Pad the tokenized sonnets\n",
    "padded_sonnets = pad_tokenized_sonnets(df['tokenized_rhyme_sonnet'].tolist(), pad_id=sp.piece_to_id('<PAD>'), max_len=max_len)\n",
    "\n",
    "# Example: print the shape of the padded tensor\n",
    "print(padded_sonnets.shape)  # Should be (batch_size, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_sonnets  # torch.LongTensor\n",
    "\n",
    "# y: X shifted left by 1 (next token prediction), pad last token with 0 or ignore_index\n",
    "y = torch.zeros_like(X)\n",
    "y[:, :-1] = X[:, 1:]\n",
    "y[:, -1] = sp.pad_id()  # or any padding token id to ignore last token prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1993,   37, 3529,  ...,    4,    4,    4],\n",
       "         [  31,   17,  214,  ...,    4,    4,    4],\n",
       "         [  49,  214,  113,  ...,    4,    4,    4],\n",
       "         ...,\n",
       "         [  31,  340,  717,  ...,    4,    4,    4],\n",
       "         [  31,  194,   12,  ...,    4,    4,    4],\n",
       "         [  31, 2269,  243,  ...,    4,    4,    4]]),\n",
       " tensor([[  37, 3529,   19,  ...,    4,    4,   -1],\n",
       "         [  17,  214,  113,  ...,    4,    4,   -1],\n",
       "         [ 214,  113, 1485,  ...,    4,    4,   -1],\n",
       "         ...,\n",
       "         [ 340,  717,   13,  ...,    4,    4,   -1],\n",
       "         [ 194,   12,  495,  ...,    4,    4,   -1],\n",
       "         [2269,  243,   13,  ...,    4,    4,   -1]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save X and y tensors to a file\n",
    "torch.save((X, y), './data/sonnets_rhymes_data.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-shakespeare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
